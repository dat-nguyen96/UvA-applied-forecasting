{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import time, sys\nimport pandas as pd\nimport numpy as np\nfrom fbprophet import Prophet\nfrom tqdm import tqdm, tnrange\nfrom multiprocessing import Pool, cpu_count\nfrom functools import partial\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings('ignore')\nwarnings.simplefilter(\"ignore\", DeprecationWarning)\nwarnings.simplefilter(\"ignore\", FutureWarning, )\n\nfuture_preds = 28","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n\n\ndf_calendar = pd.read_csv(\"../input/kaggle-afc/calendar_afcs2020.csv\")\ndf_train =  pd.read_csv(\"../input/kaggle-afc/sales_train_validation_afcs2020.csv\",index_col=0)\nevaluation = pd.read_csv(\"../input/kaggle-afc/sales_train_evaluation_afcs2020.csv\")\ndf_train.index = df_train.index.str.replace('_validation', '')\nsubmission = pd.read_csv(\"../input/kaggle-afc/sample_submission_afcs2020.csv\")\nday_cols = pd.Series([c for c in df_train.columns if c.find('d_')==0])\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_data(df_train, day_cols, indx):\n    t = df_train.loc[indx].copy()\n    t.loc[day_cols[((t.loc[day_cols]>0).cumsum()==0).values]] = np.nan\n\n    q1 = t.loc[day_cols].quantile(0.25)\n    q3 = t.loc[day_cols].quantile(0.75)\n    iqr = q3-q1\n    qm = (q3+1.5*iqr)\n    t.loc[day_cols][t.loc[day_cols]>qm] = qm\n    return t","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_calendar.index = df_calendar['d'].values\ndf_calendar['ds'] = pd.to_datetime(df_calendar['date'])\ndf_calendar['quarter'] = df_calendar['ds'].dt.quarter","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"events1 = pd.Series(df_calendar['event_name_1'].values, index=df_calendar['ds'].values).dropna()\nevents2 = pd.Series(df_calendar['event_name_2'].values, index=df_calendar['ds'].values).dropna()\nholidays = pd.DataFrame(pd.concat([events1, events2], axis=0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"holidays['ds'] = holidays.index.values\nholidays.rename({0: 'holiday'}, axis=1, inplace=True)\nholidays.reset_index(drop=True, inplace=True)\ndel events1, events2\n#%%\nprint('Cleaning data...', flush=True)\n#data = [clean_data(df_train, day_cols, i) for i in tqdm(df_train.index)]\ndata = [clean_data(df_train, day_cols, i) for i in df_train.index]\ndf_train = pd.concat(data, axis=1).T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Prepare prices')\ndf_sell_prices = pd.read_csv(\"../input/kaggle-afc/sell_prices_afcs2020.csv\")\n\ndf_sell_prices['id'] = df_sell_prices['item_id'] + '_' + df_sell_prices['store_id']\n\ndf_sell_prices = df_sell_prices.pivot(index='id', columns='wm_yr_wk', values='sell_price')\ndf_sell_prices = df_sell_prices.fillna(method='bfill', axis=1)\ndf_prices = pd.DataFrame(index=df_train.index.values)\ndf_prices = list()\nfor i in df_sell_prices.columns:\n    cols = df_calendar['d'][df_calendar['wm_yr_wk']==i]\n    t = pd.concat([df_sell_prices[i] for j in cols], axis=1)\n    t.columns = cols\n    df_prices.append(t)\ndf_prices = pd.concat(df_prices, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_prediction(indx):\n    global df_train, holidays, df_prices, df_calendar\n    changepoints=list()\n    uncertainty_samples=False\n    changepoint_prior_scale=0.1\n    changepoint_range=0.9\n    holidays_prior_scale=10\n    yearly_seasonality=2\n    weekly_seasonality=1\n    daily_seasonality=False\n    monthly_fourier_order=None\n    quarter_fourier_order=None\n    seasonality_prior_scale=10\n    seasonality_mode = 'additive'\n    \n    target = df_train.loc[indx, day_cols]\n\n    df = df_calendar.iloc[:target.shape[0]+future_preds][['ds', 'month', 'wday', 'quarter']]\n    df['y'] = target\n    df['prices'] = df_prices.loc[indx].iloc[:df.shape[0]].values\n\n    m = Prophet(growth='linear', uncertainty_samples=uncertainty_samples, changepoint_prior_scale=changepoint_prior_scale, changepoint_range=changepoint_range,\n                holidays_prior_scale=holidays_prior_scale, yearly_seasonality=yearly_seasonality,\n                daily_seasonality=daily_seasonality, weekly_seasonality=weekly_seasonality,\n                holidays=holidays, seasonality_mode=seasonality_mode, seasonality_prior_scale=seasonality_prior_scale)\n\n    if not monthly_fourier_order is None:\n        m.add_seasonality(name='monthly', period=365.25/12, fourier_order=monthly_fourier_order)\n    if not quarter_fourier_order is None:\n        m.add_seasonality(name='quarterly', period=365.25/4, fourier_order=quarter_fourier_order)#, prior_scale=15)\n\n    for reg in df.columns:\n        if reg!='ds' and reg!='y':\n            m.add_regressor(reg)\n    m.fit(df.loc[target.loc[target.first_valid_index():].index])\n\n    df.drop(['y'], axis=1, inplace=True)\n    \n    forecast = m.predict(df.iloc[-future_preds:])\n    res = forecast['yhat']\n    res.index = df.iloc[-future_preds:].index.values\n    \n    return res\n#%%","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv(\"../input/kaggle-afc/sample_submission_afcs2020.csv\", index_col=0)\n#%\n\nprint('Predicting...', flush=True)\nstart_time = time.time()\npool = Pool()\n\ntrain_indxs = df_train.index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"res = pool.map(make_prediction, train_indxs)\npool.close()\npool.join()\nend_time = time.time()\nprint('Exec speed=%.2f' %((end_time-start_time)/train_indxs.shape[0]))\n#%%\nfor j, i in enumerate(train_indxs):\n    submission.loc[i+'_validation'] = res[j].values\n#%%\nsubmission[submission<0]=0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission123.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nprice_dfs = {'CA_3': price_df[price_df['store_id'] == 'CA_3']}\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"price_dfs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"price_df = None","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"store_id","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def transform_d_dates_to_dates(d_dates):\n    return calender_df.set_index('d').loc[d_dates]['date']\n\n\ndef transform_dates_to_d_dates(dates):\n    return calender_df.set_index('date').loc[dates]['d']\n\n\ndef transform_dates_to_wm_yr_wk(dates):\n    return calender_df.set_index('date').loc[dates]['wm_yr_wk']\n\n\ndef get_avg_item_n_sold_prev_month(item_id, store_id, dates_df):\n    assert len(item_id.split('_')) == 3\n    assert store_id in ['CA_1', 'CA_2', 'CA_3', 'CA_4', 'TX_1', 'TX_2', 'TX_3', 'WI_1', 'WI_2', 'WI_3']\n    assert dates_df.shape == (1604, 14)\n    \n    d_dates_month_ago = transform_dates_to_d_dates(dates_df['date'] - pd.DateOffset(days=28))\n    assert d_dates_month_ago.shape == (dates_df.shape[0],)\n    \n    first_day = int(d_dates_month_ago.iloc[0].split('_')[1]) - 28\n    last_day = int(d_dates_month_ago.iloc[-1].split('_')[1])\n    assert first_day == 310 and last_day == 1941\n    \n    tmp = train_df[\n        (train_df['item_id'] == item_id) &\n        (train_df['store_id'] == store_id)\n    ][['d_' + str(i) for i in range(first_day, last_day + 1)]]\n    assert tmp.shape == (1, dates_df.shape[0] + 28)\n    return tmp.iloc[0].rolling(28).mean().iloc[28:].to_numpy()\n\n\ndef get_item_n_sold_year_ago(item_id, store_id, dates_df):\n    assert len(item_id.split('_')) == 3\n    assert store_id in ['CA_1', 'CA_2', 'CA_3', 'CA_4', 'TX_1', 'TX_2', 'TX_3', 'WI_1', 'WI_2', 'WI_3']\n    assert dates_df.shape == (1604, 14)\n    \n    d_dates_year_ago = transform_dates_to_d_dates(dates_df['date'] - pd.DateOffset(years=1))\n    assert d_dates_year_ago.shape == (dates_df.shape[0],)\n    \n    tmp = train_df[\n        (train_df['item_id'] == item_id) &\n        (train_df['store_id'] == store_id)\n    ][d_dates_year_ago]\n    assert tmp.shape == (1, dates_df.shape[0])\n    return tmp.iloc[0].to_numpy()\n\n\ndef get_item_price(item_id, store_id, dates_df):\n    assert len(item_id.split('_')) == 3\n    assert store_id in ['CA_1', 'CA_2', 'CA_3', 'CA_4', 'TX_1', 'TX_2', 'TX_3', 'WI_1', 'WI_2', 'WI_3']\n    assert dates_df.shape == (1604, 14)\n    \n    wm_yr_wk = transform_dates_to_wm_yr_wk(dates_df['date']).to_numpy()\n    assert wm_yr_wk.shape == (dates_df.shape[0],)\n    \n    week_to_price = price_dfs[store_id][\n        (price_dfs[store_id]['item_id'] == item_id)\n    ].set_index('wm_yr_wk')['sell_price'].to_dict()\n    \n    price = np.full(dates_df.shape[0], np.nan)\n    for i in range(wm_yr_wk.shape[0]):\n        week = wm_yr_wk[i]\n        if week in week_to_price:\n            price[i] = week_to_price[week]\n    \n    item_price_df = pd.DataFrame(data={'price': price})\n    item_price_df = item_price_df.fillna(method='ffill').fillna(method='bfill')\n\n    assert item_price_df['price'].isna().sum() == 0\n    assert item_price_df.shape == (dates_df.shape[0], 1)\n\n    # norm_price = item_price_df['price']\n    # item_price_df['price'] /= np.linspace(1.00, 1.05, num=item_price_df.shape[0])  # inflation\n    norm_price = item_price_df['price'].to_numpy()\n    \n    assert norm_price.shape == (dates_df.shape[0],)\n    return norm_price\n\n\ndef get_is_snap(item_id, store_id, dates_df):\n    assert 'FOODS' in item_id\n    assert len(item_id.split('_')) == 3\n    assert store_id in ['CA_1', 'CA_2', 'CA_3', 'CA_4', 'TX_1', 'TX_2', 'TX_3', 'WI_1', 'WI_2', 'WI_3']\n    assert dates_df.shape == (1604, 14)\n    \n    if store_id in ['CA_1', 'CA_2', 'CA_3', 'CA_4']:\n        return dates_df['snap_CA'].to_numpy()\n    elif store_id in ['TX_1', 'TX_2', 'TX_3']:\n        return dates_df['snap_TX'].to_numpy()\n    elif store_id in ['WI_1', 'WI_2', 'WI_3']:\n        return dates_df['snap_WI'].to_numpy()\n\n    assert False\n    return None\n    \n    \ndef get_week_days_features(item_id, store_id, dates_df):\n    assert len(item_id.split('_')) == 3\n    assert store_id in ['CA_1', 'CA_2', 'CA_3', 'CA_4', 'TX_1', 'TX_2', 'TX_3', 'WI_1', 'WI_2', 'WI_3']\n    assert dates_df.shape == (1604, 14)\n    \n    return pd.DataFrame(\n        index=dates_df['d'],\n        data={\n            'is_Monday': (dates_df['weekday'] == 'Monday').astype(int).to_numpy(),\n            'is_Tuesday': (dates_df['weekday'] == 'Tuesday').astype(int).to_numpy(),\n            'is_Wednesday': (dates_df['weekday'] == 'Wednesday').astype(int).to_numpy(),\n            'is_Thursday': (dates_df['weekday'] == 'Thursday').astype(int).to_numpy(),\n            'is_Friday': (dates_df['weekday'] == 'Friday').astype(int).to_numpy(),\n            'is_Saturday': (dates_df['weekday'] == 'Saturday').astype(int).to_numpy()\n        }\n    )\n    \n    \ndef get_event_features(item_id, store_id, dates_df):\n    assert len(item_id.split('_')) == 3\n    assert store_id in ['CA_1', 'CA_2', 'CA_3', 'CA_4', 'TX_1', 'TX_2', 'TX_3', 'WI_1', 'WI_2', 'WI_3']\n    assert dates_df.shape == (1604, 14)\n    \n    events_df = dates_df[['date', 'd', 'event_type_1', 'event_type_2']].copy()\n    events_df['tomorrow_event_type_1'] = events_df['event_type_1'].shift(periods=-1)\n    events_df['tomorrow_event_type_2'] = events_df['event_type_2'].shift(periods=-1)\n    \n    return pd.DataFrame(\n        index=events_df['d'],\n        data={\n            'is_today_religious': (\n                (events_df['event_type_1'] == 'Religious') |\n                (events_df['event_type_2'] == 'Religious')\n            ).astype(int).to_numpy(),\n            'is_today_national': (\n                (events_df['event_type_1'] == 'National') |\n                (events_df['event_type_2'] == 'National')\n            ).astype(int).to_numpy(),\n            'is_today_cultural': (\n                (events_df['event_type_1'] == 'Cultural') |\n                (events_df['event_type_2'] == 'Cultural')\n            ).astype(int).to_numpy(),\n            'is_today_sporting': (\n                (events_df['event_type_1'] == 'Sporting') |\n                (events_df['event_type_2'] == 'Sporting')\n            ).astype(int).to_numpy(),\n            'is_tomorrow_religious': (\n                (events_df['tomorrow_event_type_1'] == 'Religious') |\n                (events_df['tomorrow_event_type_2'] == 'Religious')\n            ).astype(int).to_numpy(),\n            'is_tomorrow_national': (\n                (events_df['tomorrow_event_type_1'] == 'National') |\n                (events_df['tomorrow_event_type_2'] == 'National')\n            ).astype(int).to_numpy(),\n            'is_tomorrow_cultural': (\n                (events_df['tomorrow_event_type_1'] == 'Cultural') |\n                (events_df['tomorrow_event_type_2'] == 'Cultural')\n            ).astype(int).to_numpy(),\n            'is_tomorrow_sporting': (\n                (events_df['tomorrow_event_type_1'] == 'Sporting') |\n                (events_df['tomorrow_event_type_2'] == 'Sporting')\n            ).astype(int).to_numpy()\n        }\n    )\n\n\ndef get_item_X_y(item, is_debug):\n    assert len(item.split('_')) == 5\n    \n    dates_df = calender_df.iloc[365:]\n    \n    item_parts = item.split('_')\n    item_id = item_parts[0] + '_' + item_parts[1] + '_' + item_parts[2]\n    store_id = item_parts[3] + '_' + item_parts[4]\n    \n    df = pd.DataFrame(\n        index=dates_df['d'].to_numpy(),\n        data={\n            'avg_item_n_sold_prev_month': get_avg_item_n_sold_prev_month(item_id, store_id, dates_df),\n            'item_n_sold_year_ago': get_item_n_sold_year_ago(item_id, store_id, dates_df),\n            'price': get_item_price(item_id, store_id, dates_df)\n        }\n    )\n    \n    if 'FOODS' in item:\n        df['is_snap'] = get_is_snap(item_id, store_id, dates_df)\n        \n    df = pd.concat([\n        df,\n        get_week_days_features(item_id, store_id, dates_df),\n        get_event_features(item_id, store_id, dates_df)\n    ], axis=1)\n    assert df.isna().sum().sum() == 0\n    \n    features_to_drop = []\n    for feature in df.columns:\n        if len(df[feature].unique()) <= 1:\n            features_to_drop.append(feature)\n    df = df.drop(features_to_drop, axis=1)\n    if is_debug:\n        print('Features', features_to_drop, 'have been dropped')\n    \n    target = train_df[\n        train_df['id'] == item + '_evaluation'\n    ][\n        ['d_' + str(i) for i in range(366, 1942)]\n    ].to_numpy()[0]\n    assert target.shape == (1576,)\n    target = np.concatenate([target, np.full(28, np.nan)])\n    df['target'] = target\n\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_item_model(item, is_debug):\n    df = get_item_X_y(item, is_debug)\n    assert df.shape[0] == 1604\n\n    X = df.drop(['target'], axis=1)\n    y = df['target']\n    \n    X_train = X.loc[['d_' + str(i) for i in range(366, 1914)]]  # train\n    y_train = y.loc[['d_' + str(i) for i in range(366, 1914)]]  # train\n    assert X_train.shape[0] == y_train.shape[0] == 1548\n    \n    X_valid = X.loc[['d_' + str(i) for i in range(1914, 1942)]]  # public test\n    y_valid = y.loc[['d_' + str(i) for i in range(1914, 1942)]]  # public test\n    X_test = X.loc[['d_' + str(i) for i in range(1942, 1970)]]  # private test\n    assert X_valid.shape[0] == y_valid.shape[0] == X_test.shape[0] == 28\n    \n    scaler = StandardScaler()\n    X_train = scaler.fit_transform(X_train)  # train\n    X_valid = scaler.transform(X_valid)  # public test\n    X_test = scaler.transform(X_test)  # private test\n    \n    model = Ridge()\n    model.fit(X_train, y_train)\n    \n    y_valid_pred = model.predict(X_valid)\n    y_valid_pred[y_valid_pred < 0] = 0\n    y_valid_pred[y_valid_pred > y.max()] = y.max()\n    \n    y_test_pred = model.predict(X_test)\n    y_test_pred[y_test_pred < 0] = 0\n    y_test_pred[y_test_pred > y.max()] = y.max()\n    \n    if is_debug:\n        plot_feature_importances(model, X.columns)\n        plot_public_test(y_valid, y_valid_pred)\n        print('PUBLIC TEST: mean_absolute_error =', mean_absolute_error(y_valid, y_valid_pred))\n    \n    return y_valid_pred, y_test_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_item_X_y('HOBBIES_1_004_CA_1', is_debug=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}