{"cells":[{"metadata":{"id":"rZNtXowbkVHc","outputId":"69c34a7e-f4ab-4b12-de5e-e03a79d0fbdd","trusted":true},"cell_type":"code","source":"\"\"\"from google.colab import drive \ndrive.mount('/content/drive',force_remount=True)\n%cd /content/drive/MyDrive/UvA SSO group 2/AFC/Project\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"id":"0yRee55xk_lR","trusted":true},"cell_type":"code","source":"\n\nfrom  datetime import datetime, timedelta\nimport gc\nimport numpy as np, pandas as pd\nimport lightgbm as lgb\n\n","execution_count":null,"outputs":[]},{"metadata":{"id":"J6nNp6b3BNuD","trusted":true},"cell_type":"code","source":"CAL_DTYPES={\"event_name_1\": \"category\", \n            \"event_name_2\": \"category\", \n            \"event_type_1\": \"category\", \n            \"event_type_2\": \"category\", \n            \"weekday\": \"category\", \n            'wm_yr_wk': 'int16', \n            \"wday\": \"int16\",\n            \"month\": \"int16\", \n            \"year\": \"int16\", \n            \"snap_CA\": \"float32\"}\n            \nPRICE_DTYPES = {\"store_id\": \"category\", \n                \"item_id\": \"category\", \n                \"wm_yr_wk\": \"int16\",\n                \"sell_price\":\"float32\" }","execution_count":null,"outputs":[]},{"metadata":{"id":"cwX1zT5ABmCa","trusted":true},"cell_type":"code","source":"pd.options.display.max_columns = 50","execution_count":null,"outputs":[]},{"metadata":{"id":"b8BBg2mMEmrx","outputId":"e2c5cff1-ffa9-4ab2-a857-9a673cd43bc1","trusted":true},"cell_type":"code","source":"\"\"\"NDAYS = 28\nMAX_LAGS = 57\nTRAINING_LAST = 1913\nFDAY = datetime(2016,4, 25) \nFDAY\n\"\"\"\nh = 28 \nmax_lags = 57\n# tr_last = 1913\ntr_last = 1913 + 28\n# fday = datetime(2016,4, 25)\nfday = datetime(2016,4, 25) + timedelta(days= 28)\nfday,tr_last,max_lags","execution_count":null,"outputs":[]},{"metadata":{"id":"wOUozKypBShS","trusted":true},"cell_type":"code","source":"def create_calendar_df():\n    calendar_df = pd.read_csv(\"../input/kaggle-afc/calendar_afcs2020.csv\", dtype = CAL_DTYPES)\n    calendar_df[\"date\"] = pd.to_datetime(calendar_df[\"date\"])\n    #for col, col_dtype in CAL_DTYPES.items():\n    #    if col_dtype == \"category\":\n    #        calendar_df[col] = calendar_df[col].cat.codes.astype(\"int16\")\n    #        calendar_df[col] -= calendar_df[col].min()\n    \n    return calendar_df\n\n\ndef create_prices_df():\n    prices_df = pd.read_csv(\"../input/kaggle-afc/sell_prices_afcs2020.csv\", dtype = PRICE_DTYPES)\n    for col, col_dtype in PRICE_DTYPES.items():\n        if col_dtype == \"category\":\n            prices_df[col] = prices_df[col].cat.codes.astype(\"int16\")\n            prices_df[col] -= prices_df[col].min()\n    prices_df['id2'] = prices_df['item_id']\n    return prices_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"l = create_prices_df()","execution_count":null,"outputs":[]},{"metadata":{"id":"qyrCpmiKIIP8","trusted":true},"cell_type":"code","source":"def create_dt(is_train = True, nrows = None, first_day = 1200):\n    start_day = max(1 if is_train else tr_last - max_lags, first_day)\n    numeric_cols = [f\"d_{day}\" for day in range(start_day, tr_last + 1)]\n    category_cols = ['id']\n    dtype = {numcol:\"float32\" for numcol in numeric_cols} \n    dtype.update({col: \"category\" for col in category_cols if col != \"id\"})\n    sales_df = pd.read_csv(\"../input/kaggle-afc/sales_train_evaluation_afcs2020.csv\", \n                     nrows = nrows, usecols = category_cols + numeric_cols, dtype = dtype)\n    \n    for col in category_cols:\n        if col != \"id\":\n            sales_df[col] = sales_df[col].cat.codes.astype(\"int16\")\n            sales_df[col] -= sales_df[col].min()\n    \n    if not is_train:\n        for day in range(tr_last + 1, tr_last + h +1):\n            sales_df[f\"d_{day}\"] = np.nan\n    \n    sales_df = pd.melt(sales_df,\n                  id_vars = category_cols,\n                  value_vars = [col for col in sales_df.columns if col.startswith(\"d_\")],\n                  var_name = \"d\",\n                  value_name = \"sales\")\n    \n    calendar_df = create_calendar_df()\n    prices_df = create_prices_df()\n    sales_df = sales_df.merge(calendar_df, on= \"d\", copy = False)\n    sales_df['id2'] = sales_df['id'].astype(str).str[:13]\n    sales_df['id2'] = sales_df['id2'].astype(\"category\")\n    sales_df['id2'] = sales_df['id2'].cat.codes.astype(\"int16\")\n    sales_df = sales_df.merge(prices_df, on = [\"id2\"], copy = False)\n    sales_df['item_id'] = sales_df['id2']\n    sales_df.pop('id2')\n    \n    return sales_df","execution_count":null,"outputs":[]},{"metadata":{"id":"w4l4QJefKWGr","trusted":true},"cell_type":"code","source":"FIRST_DAY = 350","execution_count":null,"outputs":[]},{"metadata":{"id":"SOrHPtgRIbJc","outputId":"a8f04a86-0a05-4c83-9d58-206d9f569022","trusted":true},"cell_type":"code","source":"%%time\n\ndf = create_dt(is_train=True, first_day= FIRST_DAY)\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\noverall_sales_special = df\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nprint(\"Event days in 2012\")\noverall_sales_special[(overall_sales_special.year == 2012) & ((overall_sales_special.event_name_1.notnull()) | (overall_sales_special.event_name_2.notnull()))]\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n#Function for tagging events to the preceding weekend \nevent_days_sales = overall_sales_special[((overall_sales_special.event_name_1.notnull()) | (overall_sales_special.event_name_2.notnull()))]\noverall_sales_special[\"weekend_precede_event\"] = np.nan\n\ndef update_weekend_precede_event(week_e,wday,e1,e2):\n    e2 = '_' + e2 if type(e2) == str else ''\n    drift = e1 + e2\n    if wday == 1:\n        overall_sales_special.loc[(overall_sales_special['wm_yr_wk_x']==week_e)&(overall_sales_special['wday']==1),\"weekend_precede_event\"] = drift\n    else:\n        overall_sales_special.loc[(overall_sales_special['wm_yr_wk_x']==week_e)&((overall_sales_special['wday']==1)|(overall_sales_special['wday']==2)),\"weekend_precede_event\"] = drift\n        \n_ = event_days_sales.apply(lambda row : update_weekend_precede_event(row['wm_yr_wk_x'],row['wday'],row['event_name_1'], row['event_name_2']),axis = 1)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#adding event type column\nevents.columns = [\"weekend_precede_event_type\",\"weekend_precede_event\"]\noverall_sales_special = pd.merge(overall_sales_special,events,how= \"left\",on=\"weekend_precede_event\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()\nranges = [0,1,2,3,4,5,6,7,8,9,10]\ndf.sales.groupby(pd.cut(df.sell_price, ranges)).count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"id":"g3lT9ih_Ke-r","trusted":true},"cell_type":"code","source":"def create_fea(dt):\n    lags = [7, 28]\n    lag_cols = [f\"lag_{lag}\" for lag in lags ]\n    for lag, lag_col in zip(lags, lag_cols):\n        dt[lag_col] = dt[[\"id\",\"sales\"]].groupby(\"id\")[\"sales\"].shift(lag)\n\n    wins = [7, 28]\n    for win in wins :\n        for lag,lag_col in zip(lags, lag_cols):\n            dt[f\"rmean_{lag}_{win}\"] = dt[[\"id\", lag_col]].groupby(\"id\")[lag_col].transform(lambda x : x.rolling(win).mean())\n\n    \n    \n    date_features = {\n        \n        \"wday\": \"weekday\",\n        \"week\": \"weekofyear\",\n        \"month\": \"month\",\n        \"quarter\": \"quarter\",\n        \"year\": \"year\",\n        \"mday\": \"day\",\n#         \"ime\": \"is_month_end\",\n#         \"ims\": \"is_month_start\",\n    }\n    \n#     dt.drop([\"d\", \"wm_yr_wk\", \"weekday\"], axis=1, inplace = True)\n    \n    for date_feat_name, date_feat_func in date_features.items():\n        if date_feat_name in dt.columns:\n            dt[date_feat_name] = dt[date_feat_name].astype(\"int16\")\n        else:\n            dt[date_feat_name] = getattr(dt[\"date\"].dt, date_feat_func).astype(\"int16\")\n","execution_count":null,"outputs":[]},{"metadata":{"id":"T7U0Yg7cKfET","outputId":"3a71e756-cb7f-41c2-fa66-9eec2830c901","trusted":true},"cell_type":"code","source":"%%time\n\ncreate_fea(df)\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"MDoKSkanKt4p","outputId":"b1d1fb71-e6d9-46b8-e484-dacd95d2f4f0","trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"id":"AXMv2kM_KfIz","outputId":"ab3fb3f2-2b91-45c4-d1f2-61eb26f7c1cf","trusted":true},"cell_type":"code","source":"df.dropna(inplace = True)\ndf.shape\n#(50818677, 27)","execution_count":null,"outputs":[]},{"metadata":{"id":"zUxOJnjyLKW0","trusted":true},"cell_type":"code","source":"cat_feats = ['store_id','item_id'] + [\"event_name_1\", \"event_name_2\", \"event_type_1\", \"event_type_2\"]\nuseless_cols = [\"id\",\"date\", \"sales\",\"d\", \"wm_yr_wk\", \"weekday\"]\ntrain_cols = df.columns[~df.columns.isin(useless_cols)]\nX_train = df[train_cols]\ny_train = df[\"sales\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n%%time\n\nnp.random.seed(777)\n\nfake_valid_inds = np.random.choice(X_train.index.values, 2_000_000, replace = False)\ntrain_inds = np.setdiff1d(X_train.index.values, fake_valid_inds)\ntrain_data = lgb.Dataset(X_train.loc[train_inds] , label = y_train.loc[train_inds], \n                         categorical_feature=cat_feats, free_raw_data=False)\nfake_valid_data = lgb.Dataset(X_train.loc[fake_valid_inds], label = y_train.loc[fake_valid_inds],\n                              categorical_feature=cat_feats,\n                 free_raw_data=False)# This is a random sample, we're not gonna apply any time series train-test-split tricks here!\n\n","execution_count":null,"outputs":[]},{"metadata":{"id":"S2dmJFakNLiW","outputId":"d5ba7612-87bf-424c-be61-03a94df7d479","trusted":true},"cell_type":"code","source":"del df, X_train, y_train, fake_valid_inds,train_inds ; gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"id":"sq5vY0zkPWWi","trusted":true},"cell_type":"code","source":"\n\nparams = {\n        \"objective\" : \"poisson\",\n        \"metric\" :\"rmse\",\n        \"force_row_wise\" : True,\n        \"learning_rate\" : 0.075,\n#         \"sub_feature\" : 0.8,\n        \"sub_row\" : 0.75,\n        \"bagging_freq\" : 1,\n        \"lambda_l2\" : 0.1,\n#         \"nthread\" : 4\n        \"metric\": [\"rmse\"],\n    'verbosity': 1,\n    'num_iterations' : 160,\n    'num_leaves': 128,\n    \"min_data_in_leaf\": 100,\n}\n\n","execution_count":null,"outputs":[]},{"metadata":{"id":"id8UdFbgPfd9","outputId":"c4b8279a-508e-493f-c711-c0c6b808d7dd","trusted":true},"cell_type":"code","source":"%%time\n\nm_lgb = lgb.train(params, train_data, valid_sets = [fake_valid_data], verbose_eval=20) \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m_lgb.save_model(\"model.lgb\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.rcParams['figure.figsize'] = (18.0, 4)\n%matplotlib inline \n\nfig, ax = plt.subplots(figsize=(12,8))\nlgb.plot_importance(m_lgb, max_num_features=50, height=0.8, ax=ax)\nax.grid(False)\nplt.title(\"LightGBM - Feature Importance\", fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_lag_features_for_test(dt, day):\n    # create lag feaures just for single day (faster)\n    lags = [7, 28]\n    lag_cols = [f\"lag_{lag}\" for lag in lags]\n    for lag, lag_col in zip(lags, lag_cols):\n        dt.loc[dt.date == day, lag_col] = \\\n            dt.loc[dt.date ==day-timedelta(days=lag), 'sales'].values  # !!! main\n\n    windows = [7, 28]\n    for window in windows:\n        for lag in lags:\n            df_window = dt[(dt.date <= day-timedelta(days=lag)) & (dt.date > day-timedelta(days=lag+window))]\n            df_window_grouped = df_window.groupby(\"id\").agg({'sales':'mean'}).reindex(dt.loc[dt.date==day,'id'])\n            dt.loc[dt.date == day,f\"rmean_{lag}_{window}\"] = \\\n                df_window_grouped.sales.values     ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_date_features_for_test(dt):\n    # copy of the code from `create_dt()` above\n    date_features = {\n        \"wday\": \"weekday\",\n        \"week\": \"weekofyear\",\n        \"month\": \"month\",\n        \"quarter\": \"quarter\",\n        \"year\": \"year\",\n        \"mday\": \"day\",\n    }\n\n    for date_feat_name, date_feat_func in date_features.items():\n        if date_feat_name in dt.columns:\n            dt[date_feat_name] = dt[date_feat_name].astype(\"int16\")\n        else:\n            dt[date_feat_name] = getattr(\n                dt[\"date\"].dt, date_feat_func).astype(\"int16\")\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# alphas = [1.028, 1.023, 1.018]\nalphas = [1.023, 1.018, 1.013] # We decrease alphas a little bit because we thought Trend May < Trend April\nweights = [1/len(alphas)]*len(alphas)\nsub = 0.\n\nfor icount, (alpha, weight) in enumerate(zip(alphas, weights)):\n\n    te = create_dt(False)\n    cols = [f\"F{i}\" for i in range(1,29)]\n\n    for tdelta in range(0, 28):\n        day = fday + timedelta(days=tdelta)\n        print(tdelta, day)\n        tst = te[(te.date >= day - timedelta(days=max_lags)) & (te.date <= day)].copy()\n        create_fea(tst)\n        tst = tst.loc[tst.date == day , train_cols]\n        te.loc[te.date == day, \"sales\"] = alpha*m_lgb.predict(tst) # magic multiplier by kyakovlev\n\n\n\n    te_sub = te.loc[te.date >= fday, [\"id\", \"sales\"]].copy()\n    te_sub[\"F\"] = [f\"F{rank}\" for rank in te_sub.groupby(\"id\")[\"id\"].cumcount()+1]\n    te_sub = te_sub.set_index([\"id\", \"F\" ]).unstack()[\"sales\"][cols].reset_index()\n    te_sub.fillna(0., inplace = True)\n    te_sub.sort_values(\"id\", inplace = True)\n    te_sub.reset_index(drop=True, inplace = True)\n    te_sub.to_csv(f\"submission_{icount}.csv\",index=False)\n    if icount == 0 :\n        sub = te_sub\n        sub[cols] *= weight\n    else:\n        sub[cols] += te_sub[cols]*weight\n    print(icount, alpha, weight)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.to_csv(\"fjdjjfd.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# alphas = [1.028, 1.023, 1.018]\nalphas = [1.023, 1.018, 1.013] # We decrease alphas a little bit because we thought Trend May < Trend April\nweights = [1/len(alphas)]*len(alphas)\nsub = 0.\n\nfor icount, (alpha, weight) in enumerate(zip(alphas, weights)):\n\n    te = create_dt(False)\n    cols = [f\"F{i}\" for i in range(1,29)]\n\n    for tdelta in range(0, 28):\n        day = FDAY + timedelta(days=tdelta)\n        print(tdelta, day)\n        tst = te[(te.date >= day - timedelta(days=MAX_LAGS)) & (te.date <= day)].copy()\n        create_features(tst)\n        tst = tst.loc[tst.date == day , train_cols]\n        te.loc[te.date == day, \"sales\"] = alpha*m_lgb.predict(tst) # magic multiplier by kyakovlev\n\n\n\n    te_sub = te.loc[te.date >= FDAY, [\"id\", \"sales\"]].copy()\n    te_sub[\"F\"] = [f\"F{rank}\" for rank in te_sub.groupby(\"id\")[\"id\"].cumcount()+1]\n    te_sub = te_sub.set_index([\"id\", \"F\" ]).unstack()[\"sales\"][cols].reset_index()\n    te_sub.fillna(0., inplace = True)\n    te_sub.sort_values(\"id\", inplace = True)\n    te_sub.reset_index(drop=True, inplace = True)\n    te_sub.to_csv(f\"submission_{icount}.csv\",index=False)\n    if icount == 0 :\n        sub = te_sub\n        sub[cols] *= weight\n    else:\n        sub[cols] += te_sub[cols]*weight\n    print(icount, alpha, weight)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub2 = pd.read_csv(\"../input/kaggle-afc/sales_train_evaluation_afcs2020.csv\", usecols = [\"id\"]+ [f\"d_{i}\" for i in range(1914, 1914+28)])\nsub2.rename(columns = {f\"d_{i}\": f'F{i-1913}' for i in range(1914, 1914+28)}, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub2[\"id\"] = sub2[\"id\"].str.replace(\"evaluation\", \"validation\")\nsub2[\"id\"] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sub = pd.concat([sub, sub2], axis=0, sort=False)\nsub.to_csv(\"final.csv\",index=False)\n#print(sub.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"qLB4rvH2oS5w","trusted":true},"cell_type":"code","source":"\"\"\"calendar = pd.read_csv(\"calendar_afcs2020.csv\")\nselling_prices = pd.read_csv(\"sell_prices_afcs2020.csv\")\nsample_submission = pd.read_csv(\"sample_submission_afcs2020.csv\")\nsales = pd.read_csv(\"sales_train_validation_afcs2020.csv\")\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}